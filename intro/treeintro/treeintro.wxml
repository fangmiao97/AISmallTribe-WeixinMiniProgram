<!--intro/treeintro/treeintro.wxml-->
<view class='titleArea'>
 <view class='title'>决策树介绍</view>
 <view class='text'>　　在计算机科学中，树是一种很重要的数据结构，比如我们最为熟悉的二叉查找树（Binary Search Tree），红黑树（Red-Black Tree）等，通过引入树这种数据结构，我们可以很快地缩小问题规模，实现高效的查找。</view>
 <view class='text'>　　在监督学习中，面对样本中复杂多样的特征，选取什么样的策略可以实现较高的学习效率和较好的分类效果一直是科学家们探索的目标。那么，树这种结构到底可以如何用于机器学习中呢？我们先从一个游戏开始。</view>
 <view class='text'>　　我们应该都玩过或者听过这么一种游戏：游戏中，出题者写下一个明星的名字，其他人需要猜出这个人是谁。当然，如果游戏规则仅此而已的话，几乎是无法猜出来的，因为问题的规模太大了。为了降低游戏的难度，答题者可以向出题者问问题，而出题者必须准确回答是或者否，答题者依据回答提出下一个问题，如果能够在指定次数内确定谜底，即为胜出。加入了问答规则之后，我们是否有可能猜出谜底呢？我们先实验一下，现在我已经写下了一个影视明星的名字，而你和我的问答记录如下:</view>
 <view class='intro'>•	是男的吗？Y</view>
 <view class='intro'>•	是亚洲人吗？Y</view>
 <view class='intro'>•	是中国人吗？N</view>
 <view class='intro'>•	是印度人吗？Y</view><view class='text'>•	 ……</view>
 <view class='intro'>虽然只有短短四个问题，但是我们已经把答案的范围大大缩小了，那么接下，第5个问题你应该如何问呢？我相信你应该基本可以锁定答案了，因为我看过的印度电影就那么几部。我们将上面的信息结构化如下图所示：</view>
 <image class='image' mode='aspectFit' src='https://www.chmod777.top/src/resource/tree1.png'></image>
 <view class='text'>　　在上面的游戏中，我们针对性的提出问题，每一个问题都可以将我们的答案范围缩小，在提问中和回答者有相同知识背景的前提下，得出答案的难度比我们想象的要小很多。</view>
 <view class='text'>　　回到我们最初的问题中，如何将树结构用于机器学习中？结合上面的图，我们可以看出，在每一个节点，依据问题答案，可以将答案划分为左右两个分支，左分支代表的是Yes，右分支代表的是No，虽然为了简化，我们只画出了其中的一条路径，但是也可以明显看出这是一个树形结构，这便是决策树的原型。</view>
 <view class='sec_title'>1.决策树算法简介</view>
 <view class='text'>　　我们面对的样本通常具有很多个特征，正所谓对事物的判断不能只从一个角度，那如何结合不同的特征呢？决策树算法的思想是，先从一个特征入手，就如同我们上面的游戏中一样，既然无法直接分类，那就先根据一个特征进行分类，虽然分类结果达不到理想效果，但是通过这次分类，我们的问题规模变小了，同时分类后的子集相比原来的样本集更加易于分类了。然后针对上一次分类后的样本子集，重复这个过程。在理想的情况下，经过多层的决策分类，我们将得到完全纯净的子集，也就是每一个子集中的样本都属于同一个分类。</view>
 <image class='image' mode='aspectFit' src='https://www.chmod777.top/src/resource/tree2.png'></image>
 <view class='intro'>比如上图中，平面坐标中的六个点，我们无法通过其x坐标或者y坐标直接就将两类点分开。采用决策树算法思想：我们先依据y坐标将六个点划分为两个子类（如水平线所示），水平线上面的两个点是同一个分类，但是水平线之下的四个点是不纯净的。但是没关系，我们对这四个点进行再次分类，这次我们以x左边分类（见图中的竖线），通过两层分类，我们实现了对样本点的完全分类。</view>
 <view class='intro'>由这个分类的过程形成一个树形的判决模型，树的每一个非叶子节点都是一个特征分割点，叶子节点是最终的决策分类。如下图所示：</view>
 <image class='image' mode='aspectFit' src='https://www.chmod777.top/src/resource/tree3.png'></image>
 <view class='intro'>将新样本输入决策树进行判决时，就是将样本在决策树上自顶向下，依据决策树的节点规则进行遍历，最终落入的叶子节点就是该样本所属的分类。</view>
 <view class='text'>　　上面我们介绍决策树算法的思想，可以简单归纳为如下两点：</view>
 <view class='text'>•	每次选择其中一个特征对样本集进行分类</view>
 <view class='text'>•	对分类后的子集递归进行步骤1</view>
 <view class='text'>　　看起来是不是也太简单了呢？实际上每一个步骤我们还有很多考虑的。在第一个步骤中，我们需要考虑的一个最重要的策略是，选取什么样的特征可以实现最好的分类效果，而所谓的分类效果好坏，必然也需要一个评价的指标。在上文中，我们都用纯净来说明分类效果好，那何为纯净呢？直观来说就是集合中样本所属类别比较集中，最理想的是样本都属于同一个分类。样本集的纯度可以用熵来进行衡量。 </view>
 <view class='text'>　　在信息论中，熵代表了一个系统的混乱程度，熵越大，说明我们的数据集纯度越低，当我们的数据集都是同一个类别的时候，熵为0，熵的计算公式如下：</view>
 <image class='image' mode='aspectFit' src='https://www.chmod777.top/src/resource/tree4.png'></image>
 <view class='text'>　　其中，P(xi)表示概率，b在此处取2。比如抛硬币的时候，正面的概率就是1/2，反面的概率也是1/2，那么这个过程的熵为：</view>
 <image class='image' mode='aspectFit' src='https://www.chmod777.top/src/resource/tree5.png'></image>
 <view class='text'>　　可见，由于抛硬币是一个完全随机事件，其结果正面和反面是等概率的，所以具有很高的熵。假如我们观察的是硬币最终飞行的方向，那么硬币最后往下落的概率是1，往天上飞的概率是0，带入上面的公式中，可以得到这个过程的熵为0，所以，熵越小，结果的可预测性就越强。在决策树的生成过程中，我们的目标就是要划分后的子集中其熵最小，这样后续的的迭代中，就更容易对其进行分类。</view>
</view>